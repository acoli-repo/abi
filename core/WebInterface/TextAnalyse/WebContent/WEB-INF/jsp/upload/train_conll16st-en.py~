# -*- coding: utf-8 -*-
import json
import gensim
import logging
import climate
import theanets
import numpy as np
import re
import collections
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
import random


class RelReader(object):
    """ Iterator for reading relation data """
    def __init__(self, segs, docvec=False):
        self.segs = segs
        self.docvec = docvec
    def __iter__(self):
        for file, i in zip(self.segs, range(len(self.segs))):
            for sub in [0, 1]:
                doclab = 'SEG_%d.%d' % (i, sub)
                if i % 1000 == 0:
                    print "      Reading", doclab
                text = [token for token, _ in self.segs[i][sub+1]]
                if self.docvec:
                    yield gensim.models.doc2vec.TaggedDocument(words=text, tags=i*2+sub)#[doclab])
                else:
                    yield text


class ParseReader(object):
    """ Iterator for reading parse data """
    def __init__(self, parses, docvec=False, offset=0):
        self.parses = parses
        self.docvec = docvec
        self.offset = offset
    def __iter__(self):
        i = -1
        for doc in self.parses:
            print "      Reading", doc
            for sent_i, sent in enumerate(self.parses[doc]['sentences']):
                tokens = [w for w, _ in sent['words']]
                i += 1
                if self.docvec:
                    yield gensim.models.doc2vec.TaggedDocument(words=tokens, tags=self.offset+i)#["%s_%d" % (doc, sent_i)])
                else:
                    yield tokens


def preproc(text):
    """ Text preprocessing """
    #text = text.lower()#.decode('utf-8')
    text = re.sub(r"([\.:;,\!\?\"\'])", r" \1 ", text)
    return text.split()


def build_tree(dependencies):
    """ Build tree structure from dependency list """
    tree = collections.defaultdict(lambda: [])
    for rel, parent, child in dependencies:
        tree[parent].append(child)
    return tree


def traverse(tree, node='ROOT-0', depth=0):
    """ Traverse dependency tree, calculate token depths """
    tokens = []
    for child in tree[node]:
        tokens.append((child, depth))
        tokens += traverse(tree, child, depth+1)
    return tokens


def get_token_depths(arg, doc):
    """ Wrapper for token depth calculation """
    tokens = []
    depths = {}
    for _, _, _, sent_i, token_i in arg['TokenList']:#TokenList to a certain argument in pdtb-data.json file, e.g. "TokenList": [[253, 256, 37, 1, 0], [25...
        if sent_i not in depths:#add in depth the depencies for ever sentence
            depths[sent_i] = dict(traverse(build_tree(doc['sentences'][sent_i]['dependencies'])))
        token, _ = doc['sentences'][sent_i]['words'][token_i]# get token in pdtb-parses.json:  ["House", {"CharacterOffsetBegin": 36, "CharacterOffsetEnd": 41, "Linkers": [], "PartOfSpeech": "NNP"}]
        try:
            tokens.append((token, depths[sent_i][token+'-'+str(token_i+1)]))
        except KeyError:
            tokens.append((token, None))
    print("tokens -->", tokens[:20])
    return tokens


def save_vectors(filename, inputs, outputs):
    """ Export vector features to text file """
    lookup = dict([(y,x) for x,y in label_subst.items()])
    f = open(filename, "w")
    for input, output in zip(inputs, outputs):
        f.write((lookup[output] + ' ' + ' '.join(map(str, input)) + '\n').encode('utf-8'))
    f.close()


def read_file(filename, parses):
    """ Read relation data from JSON """
    relations = []
    all_relations = []
    for row in open(filename):
        rel = json.loads(row)
        doc = parses[rel['DocID']]#get all data in pdtb-parses.json file to the same wsj document
        arg1 = get_token_depths(rel['Arg1'], doc)#of the data for Arg1/2 we onli need TokenList in get_token_depth function!
        arg2 = get_token_depths(rel['Arg2'], doc)
        context = get_context(rel, doc, context_size=1)
        #arg1 = preproc(rel['Arg1']['RawText'])
        #arg2 = preproc(rel['Arg2']['RawText'])
        # Use for word vector training
        all_relations.append((rel['Sense'], arg1, arg2))
        if rel['Type'] != 'Implicit':
            continue
        # Use for prediction (implicit relations only)
        relations.append((rel['Sense'], arg1, arg2, context))
    print(relations[:1])
    return (relations, all_relations)


def get_context(rel, doc, context_size=2):
    """ Get tokens from context sentences of arguments """
    pretext, posttext = [], []
    for context_i in reversed(range(context_size+1)):
        _, _, _, sent_i, _ = rel['Arg1']['TokenList'][0]#get sentence ID of the sentence (in everi token-sublist its the same)
        for token_i, token in enumerate(doc['sentences'][sent_i-context_i]['words']):
            token, _ = token
            if context_i == 0 and token_i >= rel['Arg1']['TokenList'][0][-1]:
                break
            pretext.append(token)
    for context_i in range(context_size+1):
        _, _, _, sent_i, _ = rel['Arg2']['TokenList'][-1]
        try:
            for token_i, token in enumerate(doc['sentences'][sent_i+context_i]['words']):
                token, _ = token
                if context_i == 0 and token_i <= rel['Arg2']['TokenList'][-1][-1]:
                    continue
                posttext.append(token)
        except IndexError:
            pass
    #print("context-->", pretext[:1])
    return (pretext, posttext)


# Data for theanets

def convert_relations(relations, label_subst, m):
    inputs = []
    outputs = []
    rel_dict = collections.defaultdict(lambda: [])
    # Convert relations: word vectors from segment tokens, aggregate to fix-form vector per segment
    for i, rel in enumerate(relations):
        senses, arg1, arg2, context = rel
        if i % 1000 == 0:
            print "Converting relation",i
        for sense in senses:
            avg = np.average([d for t, d in arg1 if d is not None])
            # Get tokens and weights
            tokens1 = [(token, 1./(2**depth)) if depth is not None else (token, 0.25) for token, depth in arg1]
            # Get weighted token vectors
            vecs = np.transpose([m[t]*w for t,w in tokens1 if t in m] + [m[t.lower()]*w for t,w in tokens1 if t not in m and t.lower() in m])
            vec1 = np.array(map(np.average, vecs))
            vec1prod = np.array(map(np.prod, vecs))
            # Get vectors for tokens in context (before arg1)
            """context1 = np.transpose([m[t] for t in context[0] if t in m] + [m[t.lower()] for t in context[0] if t not in m and t.lower() in m])
            if len(context1) == 0:
                context1avg = vec1*0
            else:
                context1avg = np.array(map(np.average, context1))
            """
            #max1 = np.array(map(max, vecs))
            #min1 = np.array(map(min, vecs))
            avg = np.average([d for t, d in arg2 if d is not None])
            # Get tokens and weights
            tokens2 = [(token, 1./(2**depth)) if depth is not None else (token, 0.25) for token, depth in arg2]
            # Get weighted token vectors
            vecs = np.transpose([m[t]*w for t,w in tokens2 if t in m] + [m[t.lower()]*w for t,w in tokens2 if t not in m and t.lower() in m])
            # Get vectors for tokens in context (after arg2)
            """context2 = np.transpose([m[t] for t in context[1] if t in m] + [m[t.lower()] for t in context[1] if t not in m and t.lower() in m])
            if len(context2) == 0:
                context2avg = vec2*0
            else:
                context2avg = np.array(map(np.average, context2))
            """
            vec2 = np.array(map(np.average, vecs))
            vec2prod = np.array(map(np.prod, vecs))
            #max2 = np.array(map(max, vecs))
            #min2 = np.array(map(min, vecs))
            #final = np.concatenate([vec1,max1,min1,vec2,max2,min2])
            #final = np.concatenate([vec1,vec2])
            ##final = np.concatenate([np.add(vec1prod,vec1), np.add(vec2prod,vec2), context1avg, context2avg])
            final = np.concatenate([np.add(vec1prod,vec1), np.add(vec2prod,vec2)])
            #final = np.concatenate([vec1prod,vec1, vec2prod,vec2])
            if len(final) > 0:
                inputs.append(final)
            else:
                continue
            outputs.append(np.array(label_subst[sense]))
    ## Theanets training from this point on
    print("input1", inputs[:5])
    inputs = np.array(inputs)
    print("inputs2", inputs[:5])
    inputs = inputs.astype(np.float32)
    print("inputs3", inputs[:5])
    outputs = np.array(outputs)
    outputs = outputs.astype(np.int32)
    return (inputs, outputs)


def convert_relations_docvec(relations, label_subst, m):
    inputs = []
    outputs = []
    rel_dict = collections.defaultdict(lambda: [])
    # Convert relations: word vectors from segment tokens, aggregate to fix-form vector per segment
    for i, rel in enumerate(relations):
        senses, arg1, arg2, context = rel
        if i % 1000 == 0:
            print "Converting relation",i
        for sense in senses:
            final = np.concatenate([m.docvecs["SEG_%d.0" % i], m.docvecs["SEG_%d.1" % i]])
            if len(final) > 0:
                inputs.append(final)#f체r jede implicit relation Bedeutung (z.B Comparison.Contrast) wird eine Liste in input eingef체gt, die vec,max,min f체r beide args enth채lt
            else:
                continue
            outputs.append(np.array(label_subst[sense]))
    ## Theanets training from this point on
    print("inputs1", inputs[:5])
    inputs = np.array(inputs)
    print("inputs2", inputs[:5])
    inputs = inputs.astype(np.float32)
    print("inputs3", inputs[:5])
    outputs = np.array(outputs)
    outputs = outputs.astype(np.int32)
    return (inputs, outputs)

"""
# Compile relation features from document vectors
inputs_all, outputs_all = convert_relations_docvec(relations)
inputs_train = inputs_all[:len(relations_train)]
inputs_dev = inputs_all[len(relations_train):]
outputs_train = outputs_all[:len(relations_train)]
outputs_dev = outputs_all[len(relations_train):]
"""

def train_theanet(method, learning_rate, momentum, decay, regularization, hidden, #hidden is a tuple, e.g. (20, 'tanh')
                  min_improvement, validate_every, patience, weight_lx, hidden_lx,
                  input_train, output_train, input_dev, output_dev, label_subst):
    """ input_train, output_train, input_dev, output_dev, label_subst are the output of the start_vector() function """ 
    split_point = int(len(input_train)*0.9)
    train_data = (input_train[:split_point], output_train[:split_point])
    valid_data = (input_train[split_point:], output_train[split_point:])
    test_data = (input_dev, output_dev)
    accs = []
    train_accs = []
    valid_accs = []
    for i in range(10):
        #exp = theanets.Experiment(theanets.Classifier, layers=(len(input_train[0]), (hidden, 'tanh'), len(label_subst)), loss='XE')
        train_data = (input_train[:split_point], output_train[:split_point])
        valid_data = (input_train[split_point:], output_train[split_point:])
        exp = theanets.Experiment(theanets.Classifier, layers=(len(input_train[0]), hidden, len(label_subst)), loss='XE')
        if weight_lx == "l1":
            if hidden_lx == "l1":
                exp.train(train_data, valid_data, optimize=method,
                                                learning_rate=learning_rate,
                                                momentum=momentum,
                                                weight_l1=decay,
                                                hidden_l1=regularization,
                                                min_improvement=min_improvement, #0.02,
                                                validate_every=validate_every, #5,
                                                patience=patience)#=5
            else:
                exp.train(train_data, valid_data, optimize=method,
                                                learning_rate=learning_rate,
                                                momentum=momentum,
                                                weight_l1=decay,
                                                hidden_l2=regularization,
                                                min_improvement=min_improvement, #0.02,
                                                validate_every=validate_every, #5,
                                                patience=patience)#=5
        else:
            if hidden_lx == "l1":   
                exp.train(train_data, valid_data, optimize=method,
                                                learning_rate=learning_rate,
                                                momentum=momentum,
                                                weight_l2=decay,
                                                hidden_l1=regularization,
                                                min_improvement=min_improvement, #0.02,
                                                validate_every=validate_every, #5,
                                                patience=patience)#=5
            else:
                exp.train(train_data, valid_data, optimize=method,
                                                learning_rate=learning_rate,
                                                momentum=momentum,
                                                weight_l2=decay,
                                                hidden_l2=regularization,
                                                min_improvement=min_improvement, #0.02,
                                                validate_every=validate_every, #5,
                                                patience=patience)#=5          
                
        confmx = confusion_matrix(exp.network.predict(test_data[0]), test_data[1])
        acc = float(sum(np.diag(confmx)))/sum(sum(confmx))
        print "acc original: ", acc
        print "acc sklear: ", accuracy_score(exp.network.predict(test_data[0]), test_data[1])
        report = classification_report(exp.network.predict(test_data[0]),test_data[1])
        print classification_report(exp.network.predict(test_data[0]),test_data[1]), "\nAverage accuracy:", acc
        print "Confusion matrix:\n", confmx
        accs.append(acc)
        print "Loop",i,"Mean accuracy", np.average(accs), np.std(accs)
        train_acc = accuracy_score(exp.network.predict(train_data[0]),train_data[1])
        train_accs.append(train_acc)
        print "Loop",i,"Mean Train-Accuracy", np.average(train_accs), np.std(train_accs)
        valid_acc = accuracy_score(exp.network.predict(valid_data[0]),valid_data[1])
        valid_accs.append(valid_acc)
        print "Loop",i,"Mean Valid-Accuracy", np.average(valid_accs), np.std(valid_accs)
        input_train = np.vstack((input_train[split_point:],input_train[:split_point]))
        output_train = np.concatenate([output_train[split_point:],output_train[:split_point]])
    f = open("Results_single_conll16st.txt", "a")
    f.writelines(["\n\n\n\nTest with parameters: \nMethod:{0},\nLearning Rate: {1},\nMomentum: {2},\nDecay: {3},\nRegularization:{4},\
                  \nHidden:{5},\nMin_improvement:{6},\
                  \nValidate_every: {7},\
                  \nPatience:{8}, \nweight_lx:{9}, \nhidden_lx{10}".format(method, learning_rate, momentum, decay, regularization, hidden, min_improvement, validate_every, patience, weight_lx, hidden_lx),
                  "\n",
                  "average acc: {0}".format(np.average(accs)), " standard deviation: {0}".format(np.std(accs)),
                  "\n",
                  "average validation acc: {0}".format(np.average(valid_accs)),  " standard deviation: {0}".format(np.std(valid_accs)),
                  "\n",
                  "average training acc: {0}".format(np.average(train_accs)),  " standard deviation: {0}".format(np.std(train_accs)),
                  "\n",
                  "last confmx: \n", confmx ])
    f.close()
    return np.average(accs), np.average(valid_accs), np.average(train_accs)
   



def start_vectors():
    """ creates vectors """
    mean = (lambda x: sum(x)/float(len(x)))
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    # Initalize semantic model (with None data)
    m = gensim.models.Word2Vec(None, size=300, window=8, min_count=3, workers=4, negative=5*0)
    #m = gensim.models.Doc2Vec(None, size=300, window=8, min_count=3, workers=4, negative=5*0)
    print "Reading data..."
    # Load parse files
    parses = json.load(open("conll16st/en-01-12-16-train/parses.json"))#"conll15st-train-dev/conll15st_data/conll15-st-03-04-15-train/pdtb-parses.json"))
    parses.update(json.load(open("conll16st/en-01-12-16-dev/parses.json")))#conll15st-train-dev/conll15st_data/conll15-st-03-04-15-dev/pdtb-parses.json")))
    #relations = []
    #all_relations = []
    
    #TEST LANWANG OUTPUT
    #(relations_train, all_relations_train) = read_file("lanwang/own_outputs/output_train.json", parses)
    #(relations_dev, all_relations_dev) = read_file("lanwang/own_outputs/output_dev.json", parses)

    (relations_train, all_relations_train) = read_file("conll16st/en-01-12-16-train/relations.json", parses)
    #"conll15st-train-dev/conll15st_data/conll15-st-03-04-15-train/pdtb-data.json", parses)
    (relations_dev, all_relations_dev) = read_file("conll16st/en-01-12-16-dev/relations.json", parses)
    #"conll15st-train-dev/conll15st_data/conll15-st-03-04-15-dev/pdtb-data.json", parses)
    relations = relations_train + relations_dev
    all_relations = all_relations_train + all_relations_dev
    # Substitution dictionary for class labels to integers
    label_subst = dict([(y,x) for x,y in enumerate(set([r[0][0] for r in relations]))])
    print "Build vocabulary..."
    m.build_vocab(RelReader(all_relations))
    #m.build_vocab(ParseReader(parses, docvec=True))
    print "Reading pre-trained word vectors..."
    m.intersect_word2vec_format("GoogleNews-vectors-negative300.bin", binary=True)
    print "Training segment vectors..."
    for iter in range(3):
        ## Training of word vectors
        m.train(ParseReader(parses))
        ## Or, training of document vectors
        #m.train(RelReader(relations,docvec=True))
        #m.train(ParseReader(parses,docvec=True,offset=len(relations*2)))
    # Test accuracy
    #print("Calculation accuracy...")
    #ac = m.accuracy("questions-words.txt")

    # Get segment vectors, e.g.:
    #m.docvecs['SEG_10.0']
    (input_train, output_train) = convert_relations(relations_train, label_subst, m)
    (input_dev, output_dev) = convert_relations(relations_dev, label_subst, m)

    return input_train, output_train, input_dev, output_dev, label_subst


    
"""
save_vectors("segvecs_avgplusprod_context_avg.txt", inputs_train, outputs_train)
save_vectors("segvecs_avgplusprod_context_avg_dev.txt", inputs_dev, outputs_dev)
"""
